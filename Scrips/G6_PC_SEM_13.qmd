---
title: "PC3-3"
author: "Grupo 6"
format: html
editor: visual
---

## GRUPO 06

Choquecahua Bendezú Carol Neyduth\
Clemente Valenzuela Brithney Coraima\
Cortez Carbonell Dariana Ysabel\
Felix Yataco Maria Fernanda\
Huaripuma Lozano Anyelina Yuli\
Larico Mamani Liz Heydi Patricia

# Cargar paquetes

```{r}
library(factoextra)
library(cluster)
library(here)
library(rio)
library(tidyverse)
```

# 1. ¿Cómo aplicaremos Machine Learning a esta sesión?

Para intentar responder preguntas de investigación a veces es necesario que se realicen muchas medidas en una misma muestra. Por ejemplo, además de recolectar variables usuales como la edad, sexo y comobilidades, podríamos recolectar tambien varios otros parámetros laboratoriales como creatinina sérica, glucosa, hemoglobina glicosilada, y varios otros adicionales. Y lo cierto es que es posible que existan patrones entre los valores de las variables. Es decir, es posible que haya una dependencia entre las variables predictoras. Por ejemplo, si un grupo de pacientes tienen insuficiencia renal aguda, algunos parámetros renales de laboratorio tendrán valores fuera del rango normal, mientras que otros parámetros, no. Un opción para aplicar técnicas convencionales es la excluir variables redundantes o variables que podamos encontrar como "no interesantes". No obstante, esto puede llevar a pérdida de información. Para estas situaciones se pueden usar técnicas de machine learning como las técnicas de agrupamiento (clustering), la cual permitan la inclusión de multiple variables y permite definir grupos de pacientes que comparten similitudes respecto a las variables incluídas.

## 1.1 Uso de las técnicas de agrupamiento para responden preguntas de investigación en salud

Las técnicas de agrupamiento son un tipo de técnica exploratoria que puede usarse con el objetivo de clasificar observaciones (por ejemplo pacientes que forman parte de una muestra) en grupos en base a su similaridad y desimilaridad de las variables. A partir de esto, obtendremos grupos cuyos individuos que pertenecen a un mismo grupo son similares pero diferentes a individuos que pertenecen a otros grupos.

Los grupos encontrados pueden ser usados para hacer predicciones o evaluar diferencias en parámetros de laboratorio. Por ejemplo, entre grupos encontrados de pacientes quienes iniciaron su tratamiento para el cáncer, podemos comparar su supervivencia, calidad de vida luego de dos años u otras medidas a partir de los clusters (grupos) encontrados.

# 2 Análisis de agrupamiento herarquico (Hierarchical Clustering)

## 2.1 Sobre el problema para esta sesión

El conjunto de datos de esta sesión contiene información de 218 pacientes que han sido evaluados en un contexto de salud reproductiva en una institución de Chincha, Perú. El conjunto de datos incluye variables numéricas como edad (años), edad relación sexual (años), número de paciente y número de hijos, que evalúan aspectos demográficos y reproductivos. El objetivo de este ejercicio es aplicar el método de agrupamiento jerárquico para identificar grupos de pacientes que comparten características similares en cuanto a su perfil demográfico y reproductivo, lo que permitirá proponer posibles categorías de riesgo o patrones diferenciados relacionados con su salud.

## 2.2 El dataset para esta sesión

Para ilustrar el proceso de análisis usaremos el conjunto de datos llamado hemo_data el cual contiene 26 observaciones con las siguientes variables: edad (años), estado marital ( variable categórica), nivel de educación ( variable categórica), religión (variable categórica), etnia (variable categórica), procedencia (variable categórica), ocupación (variable categórica), ocupación conviviente (variable categórica), ocupación conviviente (variable categórica), antecedentes familiares (variable categórica), edad relación sexual (años), parejas sexuales (número), tipo de anticonceptivo (variable categórica), antecedentes ets (variable categórica), grado de conocimiento (variable categórica), actitud (variable categórica), práctica (variable categórica), conocimiento (variable categórica), uso anticonceptivo (variable categórica).

### 2.2.1 Importando los datos

```{r}
cancer_cervical_0 <- import(here("data", "cancer_cervical.csv"))
```

## 2.3 Preparación de los datos

### 2.3.1 Solo datos numéricos

Para el análisis de agrupamiento jerárquico de esta sesión usaremos solo variables numéricas. Es posible emplear variables categóricas en esta técnica, pero esto no será cubierto aquí. El código abajo elimina las variables categóricas `Estado_marital`, `Nivel_educación` , `Religión` , `Etnia`, `Procedencia`, `Ocupación`, `Ocupación_convi`, `Antecedentes_fam`, `tipo_de_anticonceptivo`, `Antecedentes_ets`, `Grado_de_conocimiento`, `Actitud`, `Practica`, `Conocimiento`, `Usa_anticonceptivos` . `n_de_paciente` será el número para los pacientes.

```{r}
cancer_cervical_1 = cancer_cervical_0 |> 
  select(-Estado_marital, -Nivel_de_educación , -Religión, -Etnia, -Procedencia, -Ocupación, -Ocupación_convi, -Antecedentes_fam, -tipo_de_anticoceptivo, -Antecedentes_ets, -Grado_de_conocimiento, -Actitud, -Practica, -Conocimiento, -Usa_anticoceptivo ) |> 
  column_to_rownames("n_de_paciente")
```

### 2.3.2 La importancia de estandarizar

Además, es fundamental estandarizar las variables antes de realizar el análisis de agrupamiento jerárquico. Estandarizar significa transformar las variables a una escala común para hacerlas comparables entre sí. Esto es especialmente importante porque uno de los pasos clave en el método de agrupamiento consiste en calcular distancias entre los objetos (en este caso, los pacientes) a partir de las variables incluidas en el conjunto de datos. Sin embargo, dichas variables se encuentran originalmente medidas en diferentes escalas. Por ejemplo, la edad ( Edad ) se expresa en años y varía entre 25 y 55, la edad de la primera relación sexual ( Edad_relacion_sexual ) también está en años pero con un rango de 13 a 30, mientras que el número de parejas sexuales ( Parejas_sex ) y el número de hijos ( num_hijos ) son conteos que oscilan entre 0-10 y 0-4, respectivamente. Si no se realiza una estandarización previa, las variables con valores numéricos más grandes o con rangos más amplios podrían influir desproporcionadamente en el cálculo de distancias, generando agrupamientos sesgados o poco representativos de la verdadera estructura de los datos.

Para ilustrar este punto: si se agrupa a los pacientes considerando simultáneamente su edad ( Edad ) y el número de parejas sexuales ( Parejas_sex ), cabe preguntarse: ¿una diferencia de 1 año en edad es tan relevante como una diferencia de 1 pareja sexual? ¿Qué variable debería tener mayor peso en la formación de los grupos? Sin una estandarización previa, estas diferencias no serían comparables, y las variables con mayor rango numérico, como la edad (que puede variar hasta 30 años entre los pacientes), dominarían el cálculo de distancias en comparación con el número de parejas sexuales (que varía hasta 10), afectando los resultados de la clasificación. Por ello, es imprescindible aplicar una función de estandarización, como scale() en R, que transforma las variables para que tengan media cero y desviación estándar uno, permitiendo así que todas contribuyan equitativamente al análisis.

```{r}
cancer_cervical_escalado = scale(cancer_cervical_1)
```

Un vistazo a los datos antes del escalamiento:

```{r}
head(cancer_cervical_1)
```

y un vistazo después del escalamiento:

```{r}
head(cancer_cervical_escalado)
```

## 2.4 Cálculo de distancias

Dado que uno de los pasos es encontrar "cosas similares", necesitamos definir "similar" en términos de distancia. Esta distancia la calcularemos para cada par posible de objetos (pacientes) en nuestro dataset. Por ejemplo, si tuvieramos a los pacientes A, B y C, las distancia se calcularían para A vs B; A vs C; y B vs C. En R, podemos utilizar la función `dist()` para calcular la distancia entre cada par de objetos en un conjunto de datos. El resultado de este cálculo se conoce como matriz de distancias o de disimilitud.

```{r}
dist_cancer_cervical_0 <- dist(cancer_cervical_escalado, method = "euclidean")
```

## 2.4.1 (opcional) Visualizando las distancias euclidianas con un mapa de calor

Una forma de visualizar si existen patrones de agrupamiento es usando mapas de calor (heatmaps). En R usamos la función `fviz_dist()` del paquete factoextra para crear un mapa de calor.

```{r}
fviz_dist(dist_cancer_cervical_0)
```

El nivel del color en este gráfico es proporcional al valor de disimilitud entre observaciones (pacientes). Un color rojo indica una distancia con valor de 0 entre las observaciones, es decir, máxima similitud. La línea diagonal corresponde al intercepto de las mismas observaciones. Las observaciones que pertenecen a un mismo cluster (grupo) tienden a agruparse en áreas de colores similares. Sin embargo, en este gráfico, los bloques de color azul y rojo están más dispersos y entremezclados, lo que sugiere que las similitudes y diferencias entre las observaciones son más difusas y menos marcadas. Una conclusión del gráfico es que, aunque existe cierta tendencia a la formación de grupos, la separación entre ellos no es clara ni compacta, indicando una mayor heterogeneidad entre las observaciones.

## 2.5 El método de agrupamiento: función de enlace (linkage)

Método de varianza mínima de Ward.

```{r}
dist_link_cancer_cervical <- hclust(d = dist_cancer_cervical_0, method = "ward.D2")
```

## 2.7 Dendrogramas para la visualización de patrones

Los dendrogramas es una representación gráfica del árbol jerárquico generado por la función `hclust()`.

```{r}
fviz_dend(dist_link_cancer_cervical, cex = 0.7)
```

-   **Eje horizontal:**

    Representa la disimilitud o distancia entre los grupos que se están fusionando. 

-   **Eje vertical:**

    Representa los objetos o grupos que se están agrupando. 

-   **Ramas:**

    Las ramas del árbol muestran cómo se fusionan los grupos. La altura de la rama indica la distancia a la que se fusionaron los grupos. Cuanto más bajo es el punto de unión, mayor es la similitud entre los grupos

-   **Saltos o grandes distancias:**

    Los "saltos" o grandes distancias entre las ramas, indican una separación natural entre grupos. 

    En nuestro dendograma se obserban algunas ramas mas cortas que otras indicando que algunos tienen mayor similitud, sin embargo, tambien se observan ramas muy altas lo cual indica menor similitud entre estos grupos.

## 2.8 ¿Cúantos grupos se formaron en el dendrograma?

Para nuestro dendrograma definimos 3 clusters.

```{r}
fviz_dend(dist_link_cancer_cervical, 
          k = 3,
          cex = 0.5,
          k_colors = c("#8db255", "#9a57b6", "#0ed3ff"),
          color_labels_by_k = TRUE, 
          rect = TRUE)
```

En el cluster 2 se observan ramas mas cortas que las de los clursters 1 y 3 lo cual indica que en este cluster se tiene una mayor similitud en los datos.

# 3 Agrupamiento con el algoritmo K-Means

El método de agrupamiento (usando el algoritmo K-means) es una técnica de machine learning muy utilizada para dividir un conjunto de datos en un número determinado de k grupos (es decir, k clústeres), donde k representa el número de grupos predefinido por el investigador

En nuestro caso, buscamos agrupar a las pacientes del estudio de cáncer cervical en función de sus características clínicas y conductuales (como edad, edad de inicio de relaciones sexuales, número de parejas sexuales y número de hijos). La finalidad es identificar perfiles similares entre las participantes

A diferencia del agrupamiento jerárquico que exploramos anteriormente, en K-means se parte de un número de grupos fijo (predefinido) y el algoritmo redistribuye a los individuos hasta que los grupos sean estables

Aquí como funciona el algoritmo de K-Means

1.  Indicar cuántos grupos (k) se desean formar\
    En este análisis seleccionamos k = 3 para dividir a las pacientes en tres perfiles clínico-conductuales distintos.

2.  Elegir aleatoriamente k pacientes como centros iniciales\
    Por ejemplo, R selecciona 3 pacientes con diferentes valores de edad, número de parejas sexuales, etc., y estos puntos se usan como referencia inicial

3.  Asignar cada paciente al centro más cercano (usando distancia euclidiana)\
    El algoritmo compara cada paciente con los centros y los agrupa según el más próximo.

4.  Recalcular los centros de cada grupo\
    Una vez formados los grupos, se calculan los nuevos centroides como el promedio de todas las variables numéricas de cada grupo

5.  Repetir los pasos 3 y 4 hasta que los grupos se estabilicen\
    Este proceso iterativo continúa hasta que no haya cambios en la asignación de grupos o se alcance un máximo de iteraciones (usualmente 10 en R por defecto).

## 3.1 El problema y dataset para este ejercicio

Usaremos el mismo dataset de pacientes con posibles factores de riesgo para cáncer cervical, que ya empleamos en el ejercicio anterior de agrupamiento jerárquico. El objetivo sigue siendo identificar grupos homogéneos de pacientes según sus características clínicas y conductuales, como la edad, la edad de inicio de relaciones sexuales, el número de parejas sexuales y el número de hijos

## 3.2 Estimando el número óptimo de clusters

Para aplicar K-means con los datos de cáncer cervical, necesitamos definir cuántos grupos (k) formar.\
Una forma común de estimarlo es probar varios valores de k y calcular la suma de cuadrados intra-clúster (WSS).\
Luego, usamos el método del codo para identificar el punto donde agregar más grupos deja de mejorar significativamente la agrupación. En R, esto se puede hacer con la función fviz_nbclust() del paquete factoextra.

Primero escalamos los datos:

```{r}
cancer_cervical_escalado = scale(cancer_cervical_1)
```

Ahora graficamos la suma de cuadrados dentro de los gráficos

```{r}
fviz_nbclust(cancer_cervical_escalado, kmeans, nstart = 25, method = "wss") + 
  geom_vline(xintercept = 3, linetype = 2)
```

En el gráfico resultante, se observa una fuerte disminución de la WSS entre k = 1 y k = 3, lo cual indica una ganancia importante en la compacidad de los clústeres. A partir de k = 3, la pendiente del gráfico se aplana, es decir, agregar más grupos no mejora sustancialmente la calidad del agrupamiento.

Por lo tanto, el punto óptimo ("codo") se encuentra en k = 3, lo que sugiere que tres grupos es un número adecuado para segmentar de forma significativa a las pacientes según los factores analizados.

## 3.3 Cálculo del agrupamiento k-means

Dado que el resultado final del agrupamiento K-means puede variar según la selección aleatoria de los centros iniciales, se utiliza el argumento nstart = 25. Esto significa que R ejecutará el algoritmo 25 veces con diferentes asignaciones iniciales, y se quedará con la solución que tenga la menor variabilidad interna entre pacientes dentro de cada clúster.

Este enfoque ayuda a obtener una solución más estable y confiable. En este caso, usamos nstart = 25 para garantizar consistencia en los grupos formados según las variables clínicas consideradas: edad, edad de inicio de relaciones sexuales, número de parejas sexuales y número de hijos.

```{r}
set.seed(123)
km_res <- kmeans(cancer_cervical_escalado, 3, nstart = 25)
```

```{r}
km_res
```

1.  **Las medias o centros de los clústeres (Cluster means):**\
    Se obtiene una matriz con 3 filas (una por cada clúster) y 4 columnas (una por cada variable: Edad, Edad_relacion_sexual, Parejas_sex, num_hijos).\
    Estas medias están escaladas (estandarizadas), por lo que su interpretación es relativa al promedio general de la muestra. Por ejemplo:

    El clúster 1 tiene valores promedio o ligeramente altos en edad e hijos.

    El clúster 2 agrupa a pacientes más jóvenes y con menos hijos.

    El clúster 3 destaca fuertemente por un número de parejas sexuales muy por encima del promedio.

2.   **Un vector de asignación de clúster (Clustering vector):**\
    Es un vector de enteros (1, 2 o 3) que indica a qué grupo fue asignada cada paciente. Por ejemplo, el paciente 1 fue asignado al grupo 1, el paciente 2 al grupo 1, el paciente 3 al grupo 2, etc.\
    Este vector tiene la misma longitud que el número de pacientes del dataset original, y permite relacionar cada observación con su grupo correspondiente.

## 3.4 Visualización de los clústeres k-means

Al igual que en el análisis jerárquico anterior, los resultados del agrupamiento K-means pueden visualizarse mediante un gráfico de dispersión, donde cada paciente se representa como un punto, coloreado según el clúster al que pertenece.

Dado que en este análisis utilizamos cuatro variables (Edad, Edad de inicio de relaciones sexuales, Número de parejas sexuales y Número de hijos), no es posible representarlas todas directamente en un gráfico bidimensional. Por eso, se utiliza una técnica de reducción de dimensiones como el Análisis de Componentes Principales (PCA).

El PCA transforma estas cuatro variables originales en dos nuevas variables (componentes principales) que capturan la mayor parte de la variabilidad, permitiendo así representar gráficamente los clústeres en un espacio de dos dimensiones.

Para esta visualización utilizamos la función fviz_cluster() del paquete factoextra, que permite graficar los grupos generados por K-means usando los datos escalados y el resultado del modelo (km_res).

```{r}
fviz_cluster(
  km_res,
  data = cancer_cervical_escalado,  
  palette = c("#E74C3C", "#2ECC71", "#9B59B6"), 
  ellipse.type = "euclid",
  repel = TRUE,
  ggtheme = theme_minimal()
)
```

### 3.4.1 Interpretación:

En el gráfico mostrado, cada paciente se representa como un punto en el espacio, y ha sido coloreado según el grupo (clúster) asignado por el algoritmo K-means. Debido a que el análisis se realizó sobre cuatro variables cuantitativas (Edad, Edad de inicio de relaciones sexuales, Parejas sexuales, Número de hijos), y no es posible visualizarlas todas juntas, se utilizó Análisis de Componentes Principales (PCA) para reducir la información a dos dimensiones principales.

Estas nuevas dimensiones (Dim1 y Dim2) permiten observar de forma simplificada cómo se agrupan los pacientes en el espacio, destacando los patrones de similitud entre ellos. El clúster rojo (grupo 1), el clúster verde (grupo 2) y el clúster morado (grupo 3) se encuentran razonablemente separados, lo cual indica que existen diferencias claras en los perfiles clínicos y conductuales entre los grupos.

Más allá de la visualización, es necesario realizar análisis complementarios para entender mejor la utilidad de estos clústeres. Por ejemplo, se podría investigar si ciertos perfiles de pacientes tienen menor acceso a métodos anticonceptivos, o si los grupos difieren significativamente en cuanto a nivel educativo o antecedentes de ETS. Esta información podría ayudar a orientar estrategias de prevención más focalizadas en salud pública.
